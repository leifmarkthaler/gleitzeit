name: llm
version: v1
description: Large Language Model protocol with chat and completion capabilities
author: Gleitzeit Team
license: MIT
tags:
  - llm
  - ai
  - language-model
  - chat
  - completion

methods:
  llm/chat:
    description: Chat completion with message history and parameter substitution support
    parameters:
      model:
        type: string
        description: Model name to use for generation
        required: false
        default: "llama3.2"
        min_length: 1
      messages:
        type: array
        description: Array of chat messages
        required: true
        min_items: 1
        items:
          type: object
          description: A single chat message
          required: true
          properties:
            role:
              type: string
              description: Message role
              required: true
              enum: [system, user, assistant]
            content:
              type: string
              description: Message content (supports parameter substitution)
              required: true
              min_length: 1
          additional_properties: false
      temperature:
        type: number
        description: Sampling temperature (0.0 to 2.0)
        required: false
        default: 0.7
        minimum: 0.0
        maximum: 2.0
      max_tokens:
        type: integer
        description: Maximum tokens to generate
        required: false
        default: 500
        minimum: 1
        maximum: 4096
    returns:
      type: object
      description: Chat completion response
      properties:
        response:
          type: string
          description: Generated response text
          required: true
        model:
          type: string
          description: Model used for generation
          required: true
        done:
          type: boolean
          description: Whether generation is complete
          required: true
        total_duration:
          type: integer
          description: Total duration in nanoseconds
          required: false
        prompt_eval_count:
          type: integer
          description: Number of tokens in prompt
          required: false
        eval_count:
          type: integer
          description: Number of tokens generated
          required: false
      additional_properties: true
    examples:
      - description: Simple chat completion
        request:
          model: llama3.2
          messages:
            - role: user
              content: What is 2+2?
        response:
          response: "2+2 equals 4."
          model: llama3.2
          done: true
      - description: Chat with parameter substitution
        request:
          model: llama3.2
          messages:
            - role: user
              content: Calculate the square of ${number-generation.response}
        response:
          response: "64"
          model: llama3.2
          done: true

  llm/complete:
    description: Text completion with parameter substitution support
    parameters:
      model:
        type: string
        description: Model name to use for generation
        required: false
        default: "llama3.2"
        min_length: 1
      prompt:
        type: string
        description: Text prompt (supports parameter substitution)
        required: true
        min_length: 1
      temperature:
        type: number
        description: Sampling temperature (0.0 to 2.0)
        required: false
        default: 0.7
        minimum: 0.0
        maximum: 2.0
      max_tokens:
        type: integer
        description: Maximum tokens to generate
        required: false
        default: 500
        minimum: 1
        maximum: 4096
    returns:
      type: object
      description: Text completion response
      properties:
        text:
          type: string
          description: Generated text
          required: true
        model:
          type: string
          description: Model used for generation
          required: true
        done:
          type: boolean
          description: Whether generation is complete
          required: true
      additional_properties: true
    examples:
      - description: Simple text completion
        request:
          model: llama3.2
          prompt: The capital of France is
        response:
          text: "Paris."
          model: llama3.2
          done: true
      - description: Completion with parameter substitution
        request:
          model: llama3.2
          prompt: The square root of ${math-task.result.value} is
        response:
          text: "8"
          model: llama3.2
          done: true