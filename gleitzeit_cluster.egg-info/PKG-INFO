Metadata-Version: 2.4
Name: gleitzeit-cluster
Version: 0.0.1
Summary: Distributed workflow orchestration system with real-time monitoring
Author-email: Leif Markthaler <leif.markthaler@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/leifmarkthaler/gleitzeit
Project-URL: Documentation, https://github.com/leifmarkthaler/gleitzeit#readme
Project-URL: Repository, https://github.com/leifmarkthaler/gleitzeit
Project-URL: Bug Reports, https://github.com/leifmarkthaler/gleitzeit/issues
Keywords: workflow,orchestration,distributed,llm,monitoring,cluster
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: redis>=4.5.0
Requires-Dist: python-socketio>=5.8.0
Requires-Dist: aiohttp>=3.8.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: pillow>=10.0.0
Requires-Dist: pyyaml>=6.0.0
Requires-Dist: click>=8.0.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: asyncio-mqtt>=0.13.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-mock>=3.10.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: coverage>=7.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: mkdocs>=1.4.0; extra == "docs"
Requires-Dist: mkdocs-material>=9.0.0; extra == "docs"
Requires-Dist: mkdocstrings[python]>=0.20.0; extra == "docs"
Provides-Extra: monitor
Requires-Dist: rich>=13.0.0; extra == "monitor"
Provides-Extra: all
Requires-Dist: gleitzeit-cluster[dev,docs,monitor]; extra == "all"
Dynamic: license-file

# Gleitzeit

**Distributed Workflow Orchestration System**

A local-first distributed computing platform for orchestrating tasks, workflows, and batch processing. Built for LLM tasks, data processing, and complex multi-step workflows.

## Quick Install

```bash
# Install from source
pip install -e .

# Or use the installer
curl -sSL https://raw.githubusercontent.com/leifmarkthaler/gleitzeit/main/install.sh | bash
```

## Core Concepts

### üéØ Tasks
Individual units of work that can be executed independently.

- **Text Tasks**: LLM text generation and analysis
- **Vision Tasks**: Image analysis and processing  
- **Function Tasks**: Execute built-in or custom Python functions
- **HTTP Tasks**: Make API calls and process responses

### üîÑ Workflows
Orchestrate multiple tasks with dependencies and data flow.

- **Parallel execution**: Tasks run simultaneously when possible
- **Dependencies**: Control execution order with task dependencies
- **Data flow**: Pass results between tasks using `{{task_id.result}}`
- **Error handling**: Configure how workflows handle task failures

### üì¶ Batch Processing
Process multiple items efficiently with parallel execution.

- **Parallel batching**: Process lists of items simultaneously
- **Staged processing**: Multi-stage batch pipelines
- **Auto-scaling**: Automatically scale processing based on load

### üñ•Ô∏è Cluster Management
Distributed execution across multiple nodes.

- **Auto-start**: Automatically start required services
- **Service management**: Redis and executor nodes managed automatically
- **Real-time monitoring**: Track progress across distributed execution

## CLI Usage

### Basic Task Execution

```bash
# Text generation
gleitzeit run --text "Explain quantum computing" --model llama3

# Image analysis
gleitzeit run --vision photo.jpg --prompt "Describe this image" --model llava

# Function execution
gleitzeit run --function fibonacci --args n=10

# List available functions
gleitzeit functions list
```

### Workflow Management

```bash
# Start development environment
gleitzeit dev

# Run workflow from file
gleitzeit run --workflow my_workflow.yaml

# Check cluster status
gleitzeit status

# Monitor with professional interface
gleitzeit pro
```

### Batch Processing

```bash
# Data analysis functions
gleitzeit run --function random_data --args data_type=numbers count=100 min=1 max=1000
gleitzeit run --function analyze_numbers --args numbers="[1,5,10,15,20]"

# Text processing functions  
gleitzeit run --function count_words --args text="Hello world from Gleitzeit"
gleitzeit run --function extract_keywords --args text="Machine learning and AI"

# CSV data processing
gleitzeit run --function csv_filter --args file=data.csv column=age min_value=18
```

### Quick Monitoring

```bash
# Terminal monitoring (in new terminal)
gleitzeit pro

# Simple status check
gleitzeit status
```

## Requirements

- **Python 3.9+**
- **Ollama** (for LLM tasks): `curl -fsSL https://ollama.ai/install.sh | sh`
- **Models**: `ollama pull llama3` and `ollama pull llava` (for vision)

## Python API (Optional)

For programmatic workflows:

```python
import asyncio
from gleitzeit_cluster import GleitzeitCluster, Workflow, Task, TaskType, TaskParameters

async def main():
    cluster = GleitzeitCluster()
    await cluster.start()
    
    # Create workflow
    workflow = Workflow(name="Data Processing Pipeline")
    
    # Task 1: Generate data
    generate_task = workflow.add_python_task(
        name="Generate Dataset",
        function_name="fibonacci",
        kwargs={"n": 10}
    )
    
    # Task 2: Analyze data (depends on Task 1)
    analyze_task = workflow.add_python_task(
        name="Analyze Numbers", 
        function_name="analyze_numbers",
        kwargs={"numbers": "{{Generate Dataset.result}}"},
        dependencies=["Generate Dataset"]
    )
    
    # Task 3: Summarize with LLM (depends on Task 2)
    summary_task = workflow.add_text_task(
        name="Generate Summary",
        prompt="Create a summary of this analysis: {{Analyze Numbers.result}}",
        model="llama3",
        dependencies=["Analyze Numbers"]
    )
    
    # Submit and monitor workflow
    workflow_id = await cluster.submit_workflow(workflow)
    
    # Wait for completion
    while True:
        status = await cluster.get_workflow_status(workflow_id)
        print(f"Progress: {status['completed_tasks']}/{status['total_tasks']}")
        
        if status["status"] == "completed":
            print("Results:", status["task_results"])
            break
        elif status["status"] == "failed":
            print("Workflow failed:", status.get("error"))
            break
            
        await asyncio.sleep(1)
    
    await cluster.stop()

asyncio.run(main())
```

### Batch Processing

```python
import asyncio
from gleitzeit_cluster import GleitzeitCluster, Workflow, Task, TaskType, TaskParameters

async def batch_text_analysis():
    cluster = GleitzeitCluster()
    await cluster.start()
    
    # Batch processing workflow
    workflow = Workflow(name="Batch Text Analysis")
    
    texts = [
        "Artificial intelligence is transforming industries",
        "Climate change requires immediate action",  
        "Space exploration opens new frontiers"
    ]
    
    # Create parallel analysis tasks
    for i, text in enumerate(texts):
        workflow.add_python_task(
            name=f"Analyze Text {i+1}",
            function_name="count_words",
            kwargs={"text": text}
        )
        
        workflow.add_python_task(
            name=f"Extract Keywords {i+1}",
            function_name="extract_keywords", 
            kwargs={"text": text, "max_keywords": 3}
        )
    
    # Aggregation task
    workflow.add_text_task(
        name="Generate Report",
        prompt='''Create a report from these analyses:
        Text 1 words: {{Analyze Text 1.result}}, keywords: {{Extract Keywords 1.result}}
        Text 2 words: {{Analyze Text 2.result}}, keywords: {{Extract Keywords 2.result}}
        Text 3 words: {{Analyze Text 3.result}}, keywords: {{Extract Keywords 3.result}}''',
        model="llama3",
        dependencies=[f"Analyze Text {i+1}" for i in range(3)] + [f"Extract Keywords {i+1}" for i in range(3)]
    )
    
    # Execute batch workflow
    workflow_id = await cluster.submit_workflow(workflow)
    
    # Monitor progress
    while True:
        status = await cluster.get_workflow_status(workflow_id)
        if status["status"] in ["completed", "failed"]:
            break
        await asyncio.sleep(1)
    
    if status["status"] == "completed":
        print("Batch Results:")
        for task_name, result in status["task_results"].items():
            print(f"  {task_name}: {result}")
    
    await cluster.stop()

asyncio.run(batch_text_analysis())
```

## What Makes This Different?

- **Local-first**: Everything runs on your machine
- **No API keys**: No cloud dependencies or costs
- **Workflow orchestration**: Chain tasks with dependencies and data flow
- **Built-in functions**: 30+ secure functions for data processing
- **Privacy**: Your data never leaves your computer
- **Simple**: Just install and run commands
- **Extensible**: 30+ built-in functions + Python API

## Status

- **Version**: 0.0.1 (Alpha)
- **License**: MIT  
- **Python**: 3.9+ required
- **Platform**: macOS, Linux (Windows support planned)

## Contributing

This project is in active development. Contributions welcome!

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

**Repository**: https://github.com/leifmarkthaler/gleitzeit
