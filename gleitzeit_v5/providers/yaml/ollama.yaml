name: ollama-provider
protocol: llm
version: v1
description: Ollama local LLM provider for chat and completion tasks
capabilities:
  - llm_chat
  - llm_completion
  - local_inference
  - parameter_substitution

connection:
  type: http
  base_url: http://localhost:11434
  timeout: 60
  retries: 3
  health_check:
    endpoint: /api/tags
    method: GET
    expected_status: 200

authentication:
  type: none  # Ollama typically runs without authentication locally

metadata:
  provider_type: local
  supports_streaming: true
  default_model: llama3.2
  available_models:
    - llama3.2
    - llama3.2:13b
    - codellama
    - mistral
  hardware_requirements:
    min_ram_gb: 8
    recommended_ram_gb: 16
    gpu_support: true
  features:
    - chat_completion
    - text_completion
    - parameter_substitution
    - context_memory
  limitations:
    max_context_length: 4096
    max_tokens_per_request: 4096
    rate_limit: none

examples:
  - name: simple_chat
    description: Basic chat completion
    method: llm/chat
    parameters:
      model: llama3.2
      messages:
        - role: user
          content: Hello, how are you?
    expected_response_fields:
      - response
      - model
      - done

  - name: code_generation
    description: Generate Python code
    method: llm/chat
    parameters:
      model: codellama
      messages:
        - role: user
          content: Write a Python function to calculate fibonacci numbers
    expected_response_fields:
      - response
      - model
      - done

  - name: parameter_substitution
    description: Chat with parameter substitution
    method: llm/chat
    parameters:
      model: llama3.2
      messages:
        - role: user
          content: "Analyze this number: ${previous-task.result.value}"
    expected_response_fields:
      - response
      - model
      - done