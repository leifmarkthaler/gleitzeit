# Mixed Provider Workflow Example
# Demonstrates using both LLM and Python execution

name: "Mixed Provider Workflow"
description: "Combines LLM tasks with Python code execution"
version: "1.0"

# Workflow configuration
timeout: 180
wait_for_completion: true

# Tasks using different providers
tasks:
  - id: "generate_numbers"
    method: "llm/chat"
    priority: 3
    parameters:
      model: "llama3.2"
      messages:
        - role: "user"
          content: "Generate 5 random numbers between 1 and 100. Reply with just the numbers separated by commas, nothing else."

  - id: "calculate_stats"
    method: "python/execute"
    priority: 2
    dependencies: ["generate_numbers"]
    parameters:
      code: |
        # Parse the numbers from the LLM response
        numbers_str = "${generate_numbers.response}"
        numbers = [int(x.strip()) for x in numbers_str.split(',')]
        
        # Calculate statistics
        import statistics
        mean = statistics.mean(numbers)
        median = statistics.median(numbers)
        stdev = statistics.stdev(numbers) if len(numbers) > 1 else 0
        
        result = {
            "numbers": numbers,
            "count": len(numbers),
            "mean": round(mean, 2),
            "median": median,
            "standard_deviation": round(stdev, 2),
            "min": min(numbers),
            "max": max(numbers)
        }
        
        print(f"Statistics for {numbers}:")
        print(f"Mean: {result['mean']}")
        print(f"Median: {result['median']}")
        print(f"Std Dev: {result['standard_deviation']}")
      timeout: 30

  - id: "summarize_results"
    method: "llm/chat"
    priority: 1
    dependencies: ["calculate_stats"]
    parameters:
      model: "llama3.2"
      messages:
        - role: "user"
          content: "Summarize these statistical results in a friendly, easy-to-understand way: ${calculate_stats.result}"